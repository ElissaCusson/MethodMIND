{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245002df-17c3-46dc-ac0b-5d0f733648df",
   "metadata": {},
   "source": [
    "### How to run TintLlama ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a366c7-b248-4fbe-bf8e-a81ad530f978",
   "metadata": {},
   "source": [
    "Define environnement variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec59d3f-b261-44b1-a811-c12c7c511757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75766de8-2742-441a-b513-d913ee14d03d",
   "metadata": {},
   "source": [
    "Install the llama cpp-python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a78648-2c63-4717-bf6e-3ac34282e8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from llama-cpp-python) (2.0.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba1ee02-fa87-4b7f-a9a6-b7f122ca18ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gugly/.pyenv/versions/3.10.6/envs/MethodMIND/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a6bbd-17e2-44ac-b4e0-6b084453a333",
   "metadata": {},
   "source": [
    "Download the right Llama from the Huggingface client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd1f697-8a89-42d6-bd35-baecbf72ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf' to '.cache/huggingface/download/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf.9fecc3b3cd76bba89d504f29b616eedf7da85b96540e490ca5824d3f7d2776a0.incomplete'\n",
      "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf: 100%|██| 669M/669M [00:31<00:00, 21.4MB/s]\n",
      "Download complete. Moving file to tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
      "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --local-dir ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e881b03-5c3b-4455-ba66-ae1cdf615d34",
   "metadata": {},
   "source": [
    "Import the libraray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a01a34-c2c6-45a0-a72a-c266d0aee88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af14cd-6f1a-4251-a81a-8147a9b73f69",
   "metadata": {},
   "source": [
    "Model parameters, explanation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3025a77-daae-41b7-a675-872ae70e7961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 200 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   636.18 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    44.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   44.00 MiB, K (f16):   22.00 MiB, V (f16):   22.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   148.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# For Mac OS M3, use\n",
    "llm = Llama(model_path=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "            n_ctx=2048,\n",
    "            n_threads=8,\n",
    "            n_gpu_layers=0)\n",
    "\n",
    "# For Intel system, with no GPU, use:\n",
    "#llm = Llama(\n",
    "#    model_path=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",  # Path to your TinyLlama model\n",
    "#    n_ctx=2048,  # Context window size\n",
    "#    n_threads=4,  # Number of CPU threads to use\n",
    "#    n_batch=32,    # Batch size for processing tokens (lower if memory is limited)\n",
    "#    n_gpu_layers=0  # If you do not have GPU\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246707df-778a-42b9-9060-f0a2f02b1485",
   "metadata": {},
   "source": [
    "## Key Parameters Explained\n",
    "### model_path:\n",
    "\n",
    "Path to the quantized TinyLlama model file. Ensure this file is accessible on your Ubuntu machine.\n",
    "\n",
    "### n_ctx (Context Window):\n",
    "Specifies the maximum number of tokens that the model can handle at once. Set to 2048 for optimal balance between memory usage and performance.\n",
    "\n",
    "#### n_threads (CPU Threads):\n",
    "Set this to the number of physical cores or threads available on your Intel CPU.\n",
    "Example:\n",
    "If your Intel CPU has 4 cores/8 threads, use n_threads=4 (leave some headroom for other processes).\n",
    "For a high-end CPU with 8 cores/16 threads, you can increase n_threads to 8 or 12.\n",
    "\n",
    "### n_batch (Batch Size):\n",
    "\n",
    "Controls how many tokens are processed at a time. Lower values reduce memory usage but may slow down processing.\n",
    "Recommended: Start with n_batch=32. If you run into memory issues, reduce it to 16 or 8.\n",
    "\n",
    "### n_gpu_layers:\n",
    "\n",
    "If you have a GPU:\n",
    "\n",
    "Use this parameter to speed up processing by leveraging GPU capabilities.\n",
    "Balance the number of layers offloaded with the available GPU memory.\n",
    "For TinyLlama, 35 is a good value\n",
    "\n",
    "If you don’t have a GPU:\n",
    "\n",
    "Set n_gpu_layers=0 or omit it entirely (it’s irrelevant since there’s no GPU to use)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a44b9d-9ab9-490e-a7fd-b7c30a8b1162",
   "metadata": {},
   "source": [
    "Full context given by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29789df-d101-4dd6-8dac-b1176636d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_role= \"\"\"You are a highly knowledgeable and empathetic lecturer specializing in mental diseases and mental health. Your role is to provide detailed, accurate, and evidence-based explanations about mental diseases based on the raw data you receive.\n",
    "The raw data will typically include abstracts of research papers, clinical studies, or other medical documents. Analyze the data provided to you and synthesize the information into a clear and concise explanation. Your responses must:\n",
    "1. Summarize the main points of the abstract, highlighting key findings or relevant data.\n",
    "2. Explain concepts in a way suitable for a professional audience, such as medical students, researchers, or mental health practitioners, while remaining approachable for non-experts if necessary.\n",
    "3. Use medical terminology appropriately, but ensure definitions or explanations are provided for complex terms.\n",
    "4. Organize responses into structured formats when appropriate (e.g., bullet points, numbered lists, or sections like \"Background,\" \"Findings,\" \"Implications\").\n",
    "5. Provide references to the data where relevant (e.g., 'According to the abstract, the study found...').\n",
    "6. Adopt a professional, empathetic tone while avoiding judgmental language or bias.\n",
    "If the input data is unclear or insufficient, request clarification or more context to ensure an accurate response.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b110fe-f10b-48e9-a184-8415f5ad06ae",
   "metadata": {},
   "source": [
    "Optional one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ebd02b-7023-4619-a35a-e2a7a6c1afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_role= \"You are a highly knowledgeable and empathetic lecturer specializing in mental diseases\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa244d38-1275-40e5-bb32-f01fded94015",
   "metadata": {},
   "source": [
    "If you wish an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987c302e-99c2-4f52-a050-cab4415b9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_content=input(\"What do you want ?\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84dfa8d1-6d70-4b21-8f99-eb8b028c83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content=\"What do you know about Alzeihmer ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2916952d-0b84-4bdc-ab28-d7ddc3cf71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4829.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   339 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22710.12 ms /   597 tokens\n"
     ]
    }
   ],
   "source": [
    "response=llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": system_role\n",
    "\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": user_content\n",
    "        }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea2e77a-f0ad-4760-b0a9-619e1f104ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "\n",
      "I do not have the latest information on alzeihmer. However, I can provide you with some general information about alzeihmer, a type of dementia that is characterized by memory loss, impaired judgment, and behavioral changes. Alzeihmer is a subtype of dementia that is often associated with other forms of dementia, such as vascular dementia, frontotemporal dementia, and frontotemporal lobar degeneration. It is also known as the \"white matter disease\" because the brain tissue in the white matter regions of the brain, which are involved in memory and cognitive function, is affected. Alzeihmer is considered a progressive disease, meaning it worsens over time. It is often diagnosed in older adults, but it can affect younger individuals as well. Alzeihmer is often associated with other forms of dementia, such as vascular dementia, frontotemporal dementia, and frontotemporal lobar degeneration. It is also associated with other conditions, such as Parkinson's disease, Alzheimer's disease, and frontotemporal lobar degeneration.\n"
     ]
    }
   ],
   "source": [
    "content = response['choices'][0]['message']['content']\n",
    "\n",
    "print(\"Generated Response:\\n\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9dd83c-1276-4951-989d-8616ce3f7360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
